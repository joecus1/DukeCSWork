{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3\n",
    "__CS 216, Everything Data, Spring 2020__\n",
    "\n",
    "__DUE: Monday Feb. 10 by 4:40 pm (class time)__\n",
    "\n",
    "In this assignment, you will investigate record linkage or matching. In HW 2, you frequently joined tables from the `Congress` database in order to form queries. In that database, we had the luxury of unique IDs such that we could match records between different tables based on these IDs. As discussed in Lecture 3, frequently we wish to merge datasets from multiple sources that may not include exactly identical unique IDs for performing join operations. In this assignment, you will practice techniques for dealing with this problem. \n",
    "\n",
    "You will include all of your answers for this assignment within this notebook. You will then convert your notebook to a .pdf and a .py file to submit to gradescope (submission instructions are included at the bottom).\n",
    "\n",
    "Please take note of the [course collaboration policy](https://sites.duke.edu/compsci216s2020/policies/). You may work alone or with a single partner. If you work with a partner, you may not split up the assignment; you should work together in-person or complete parts independently and come together to discuss your solutions. In either case, you are individually responsible for your work, and should understand everything in your submission.\n",
    "\n",
    "**Joe Cusano** (jgc28) and **Pierce Forte** (phf7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Getting Started\n",
    "We will work with the `restaurants_a.csv` and `restaurants_b.csv` datasets for this assignment. Each contain five columns: id (a numeric index serving as a unique id, not correlated across the datasets), name (of the restaurant), address (the street address), city, and type (the type of restaurant). You can access `.csv` files for these datasets from the box folder containing this Jupyter notebook. Make sure that you download the files and have them in the same working directory as your notebook. \n",
    "\n",
    "This entire assignment can be completed using only `Python`, but you are welcome to use `Pandas` or any other standard libraries to help you.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>belvedere  the</td>\n",
       "      <td>9882 little santa monica blvd.</td>\n",
       "      <td>beverly hills</td>\n",
       "      <td>pacific new wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>triangolo</td>\n",
       "      <td>345 e. 83rd st.</td>\n",
       "      <td>new york</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>broadway deli</td>\n",
       "      <td>3rd st. promenade</td>\n",
       "      <td>santa monica</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>lettuce souprise you (at)</td>\n",
       "      <td>3525 mall blvd.</td>\n",
       "      <td>duluth</td>\n",
       "      <td>cafeterias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>otabe</td>\n",
       "      <td>68 e. 56th st.</td>\n",
       "      <td>new york</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       name                         address  \\\n",
       "0   0             belvedere  the  9882 little santa monica blvd.   \n",
       "1   1                  triangolo                 345 e. 83rd st.   \n",
       "2   2              broadway deli               3rd st. promenade   \n",
       "3   3  lettuce souprise you (at)                 3525 mall blvd.   \n",
       "4   4                      otabe                  68 e. 56th st.   \n",
       "\n",
       "            city              type  \n",
       "0  beverly hills  pacific new wave  \n",
       "1       new york           italian  \n",
       "2   santa monica          american  \n",
       "3         duluth        cafeterias  \n",
       "4       new york             asian  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_a = pd.read_csv('restaurants_a.csv')\n",
    "df_b = pd.read_csv('restaurants_b.csv')\n",
    "df_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Record Linkage and Similarity Measures\n",
    "When multiple datasets are drawn from different sources, it is frequently the case that they do not have a common unique identification system like we were lucky to have in the `congress` database from homework 2. That can make it difficult to join the datasets, which requires more complex forms of record linkage. \n",
    "\n",
    "The simplest approach to record linkage is simply to pick an attribute and join on that. In many cases this is inadequate: there may be mispellings of names, nicknames, or other slight variations such that some records that should be merged do not end up merged. This frequently leaves us with duplicate records. \n",
    "\n",
    "For the remainder of the assignment, we will think of the task of record linkage as that of determining duplicates after performing an outer join. __Your task will be to identify the pairs of records between the `restaurants_a` and `restaurants_b` datasets that should be merged because they refer to the same entity.__ In the end, you will produce a set of such pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem A: Using Edit Distance\n",
    "One way to catch small spelling errors (substituting i and e, swapping the order of letters, omitting an apostrophe, etc.) is to compute the edit distance between strings. Recall from Lecture 3 that the edit distance is the number of single character delete, insert, and substitute operations necessary to transform one string into another. We have included code to compute the edit distance between two strings for you in the file `hw3_utils.py`, you can import the function and see an example of it's use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from hw3_utils import edit_dist\n",
    "print(edit_dist(\"hello!\", \"hallo!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For problem A, use `edit_dist` to try to find likely mispellings of restaurant names between `df_a` and `df_b`. Specifically, print the names of all pairs of records (one from `df_a` and the other from `df_b`) such that the two names have edit distance at least 1 and at most 2 (note that if two strings have edit distance 0, they are exactly the same; you do not need to print these for problem A). Among these, which pairs do you think are \"real\" mispellings, and which do you think might actually be different restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'orangerie, l orangerie\n",
      "indigo coast grill, indigo coastal grill\n",
      "boulavard, boulevard\n",
      "drago, spago\n",
      "felidia, filidia\n",
      "march, marichu\n",
      "mesa grill, sea grill\n",
      "uncle nicks, uncle nick's\n"
     ]
    }
   ],
   "source": [
    "for name1 in df_a[\"name\"]:\n",
    "    for name2 in df_b[\"name\"]:\n",
    "        d = edit_dist(name1, name2)\n",
    "        if d >= 1 and d <= 2:\n",
    "            print(name1 + \", \" + name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Misspellings:**  \n",
    "- l'orangerie and l orangerie  \n",
    "- indigo coast grill and indigo coastal grill  \n",
    "- boulavard and boulevard  \n",
    "- felidia and filidia  \n",
    "- uncle nicks' and unckl nick's  \n",
    "\n",
    "**Different:**  \n",
    "- drago and spago  \n",
    "- march and marichu  \n",
    "- mesa grill and sea grill  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem B: Trigram-Decomposition\n",
    "Another way to catch spelling and representation errors we discussed in class is to compute the jaccard similarity of strings as represented by their trigram set decompositions. Recall that the trigram set decomposition of a string is the set of all 3 character contiguous substrings. We will also include the 1 and 2 character contiguous substrings at the beginning and end of the string.  For example, the trigram decomposition of the string \"hello!\" is {'h', 'he', 'hel', 'ell', 'llo', 'lo!', 'o!', '!'}. For problem b, implement a function that computes the trigram decomposition of a string, and verify that it produces the correct output on the example \"hello!\" (note that the order in a set does not matter, so {'!', 'o!', 'lo!',...} would be equally correct). \n",
    "\n",
    "If you are still learning Python, you might find the following tips helpful in completing problem B.\n",
    "- Strings can be indexed and sliced in Python as if they were arrays of characters. So, for example, if you want to get the first character of `my_str = \"hello!\"`, you could write `my_str[0]`, which would return \"h\". Similarly, you can use standard slicing syntax to get substrings, for example, `my_str[0:3]` would return \"hel\". For more about Python strings, see https://docs.python.org/3.7/tutorial/introduction.html#strings.\n",
    "- Sets are a basic data structure in Python. You can initialize an empty by, for example, `my_set = set()`. Sets can contain arbitrary data types (strings, of course, are what we are interested in here). You can add an element to a set using `add`, for example, `my_set.add('h')` would result in our empty set from before being updated to include the string \"h\". For more about Python sets, see https://docs.python.org/3.7/tutorial/datastructures.html#sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!', 'ell', 'h', 'he', 'hel', 'llo', 'lo!', 'o!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tri(string):\n",
    "    my_set = set()\n",
    "\n",
    "    for i in range(len(string)):\n",
    "        left = i\n",
    "        right = i\n",
    "        if i == 0 or i == (len(string) -1):\n",
    "            my_set.add(string[i])\n",
    "        if i == (len(string) - 1):\n",
    "            right = i +1\n",
    "        if i - 1 >= 0:\n",
    "            left = i - 1\n",
    "        if i + 1 <= (len(string) -1):\n",
    "            right = i + 2\n",
    "        my_set.add(string[left:right])\n",
    "    return my_set\n",
    "\n",
    "tri(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem C: Jaccard Similarity\n",
    "Once we can compute the trigram set decomposition of a string, we can use that to compute the Jaccard similarity between two strings. Recall from lecture 3 that the Jaccard similarity of two sets A and B is the ratio of the size of the intersection of A and B over the size of the union of A and B. In our context, the sets will be the trigram decomposotions we get for two different strings. For problem C, implement a function that computes the Jaccard similarity between two strings. Verify that your function correctly computes that the Jaccard similarity of the strings \"hello!\" and \"hallo!\" is 5/11 (~0.455).\n",
    "\n",
    "Note that Python sets have built in functions for computing intersections and unions; feel free to take advantage: https://docs.python.org/3.7/tutorial/datastructures.html#sets. The function `len` will also give you the size of a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "def jac(string1, string2):\n",
    "    set1 = tri(string1)\n",
    "    set2 = tri(string2)\n",
    "    union = set1.union(set2)\n",
    "    inter = set1.intersection(set2)\n",
    "    return len(inter)/len(union)\n",
    "\n",
    "print(jac(\"hello!\", \"hallo!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem D: Using Jaccard Similarity\n",
    "For problem D, use your function for computing Jaccard similarity from problem C to try to find likely mispellings of restaurant names between `df_a` and `df_b`. Specifically, print the names of all pairs of records (one from `df_a` and the other from `df_b`) such that the two names have Jaccard similarity at least 0.5 and strictly less than 1 (note that if two strings have Jaccard similarity of 1, they are (likely) exactly the same; you do not need to print these for problem D). State the mispellings you found in problem A that you do not find with this method, and also the mispellings you find with this method that you did not find in problem A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'orangerie, l orangerie\n",
      "indigo coast grill, indigo coastal grill\n",
      "boulavard, boulevard\n",
      "tillerman  the, tillerman\n",
      "gotham bar & grill, gotham bar and grill\n",
      "pano's & paul's, pano's and paul's\n",
      "ritz-carlton cafe (buckhead), cafe  ritz-carlton  buckhead\n",
      "felidia, filidia\n",
      "uncle nicks, uncle nick's\n"
     ]
    }
   ],
   "source": [
    "for name1 in df_a[\"name\"]:\n",
    "    for name2 in df_b[\"name\"]:\n",
    "        d = jac(name1, name2)\n",
    "        if d < 1 and d >= 0.5:\n",
    "            print(name1 + \", \" + name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the following lists do not include names that we believe are different restaurants, only actual mispelllings**\n",
    "\n",
    "**Mispellings from this:**  \n",
    "- l'orangerie and l orangerie  \n",
    "- indigo coast grill and indigo coastal grill  \n",
    "- boulavard and bouulevard  \n",
    "- tiller man the and tillerman  \n",
    "- gotham bar and grill and gotham bar & grill  \n",
    "- pano's & paul's and pano's and paul's  \n",
    "- ritz-carlton cafe (buckhead) and cafe ritz-carlton buckhead  \n",
    "- felidia and filidia  \n",
    "- uncle nicks' and uncle nick's  \n",
    "\n",
    "**Mispellings from edit_dist:**  \n",
    "- l'orangerie and l orangerie  \n",
    "- indigo coast grill and indigo coastal grill  \n",
    "- boulavard and boulevard  \n",
    "- felidia and filidia  \n",
    "- uncle nicks' and unckl nick's  \n",
    "\n",
    "**Mispellings in A and not in this:**  \n",
    "None\n",
    "\n",
    "**Mispellings from this and not in A:**  \n",
    "- tiller man the and tillerman  \n",
    "- gotham bar and grill and gotham bar & grill  \n",
    "- pano's & paul's and pano's and paul's  \n",
    "- ritz-carlton cafe (buckhead) and cafe ritz-carlton buckhead  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Record Linking the Restaurant Datasets\n",
    "Now you will compute your overall matching of records that should be merged between the two datasets. __You should format your answer as a *set* of pairs of ids__, one from `df_a` and another from `df_b`, such that you predict the corresponding records should be merged. For example, `{(661, 801), (228, 388), (304, 735), (101, 102)}` identifies that the records with ids 661 and 801 should be merged, 228 and 388 should be merged, 304 and 735 should be merged, and 101 and 102 should be merged. The ids are unique between the datasets, so you don't have to worry about a given id appearing in both. You also don't have to worry about the order of the pairs: `(661, 801)` will be treated equivalently as `(801, 661)` for the purpose of quantifying error (and if you include both orders, the duplicate will be removed when quantifying error). \n",
    "\n",
    "A couple of tips if you are new to using Python:\n",
    "- Note that Python has sets as a built-in data structure. See instructions for problems B and C or see https://docs.python.org/3.7/tutorial/datastructures.html#sets for more info.\n",
    "- Python also supports tuples as a built-in data structure. In our case, we are particularly interested in pairs. Say you have two ids: `id_a` and `id_2`, and you want to store the pair. You can do so simply by `id_pair = (id_a, id_b)`. You can subsequently index the pair as if it was a list. For example, `id_pair[0]` would return `id_a`. See https://docs.python.org/3.7/tutorial/datastructures.html#tuples-and-sequences for more info.\n",
    "- The format of the answer as given above is thus just a set of tuples. You can initialize an empty set with `my_set = set()`, and if you have a pair `id_pair = (id_a, id_b)`, you can add it to the set with `my_set.add(id_pair)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Error\n",
    "First, we want to make sure we understand how to quantify how \"good\" a particular matching is. Recall from Lecture 3 that we discussed the *precision* and *recall* of a matching as measures of quality. \n",
    "- The **precision** is the fraction of our predictions that really should be merged.\n",
    "- The **recall** is the fraction of the set of true pairs of records that go together that we predict should be merged.\n",
    "\n",
    "It is easy to do well on just one of these dimensions, but achieving high precision and recall simultaneously can be difficult. The *F1 score* is the harmonic mean of precision and recall. More precisely, $F1 = 2 \\cdot \\frac{\\mbox{precision} \\cdot \\mbox{recall}}{\\mbox{precision} + \\mbox{recall}}$. Getting a high F1 score requires doing well on both measures simultaneously.\n",
    "\n",
    "We have included code to compute the precision, recall, and F1 score of a set of pairs of formatted as specified above. Below we import them and show the output on the example matching from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.75\n",
      "Recall:  0.04918032786885246\n",
      "F1 score:  0.0923076923076923\n"
     ]
    }
   ],
   "source": [
    "from hw3_utils import precision, recall, f1_score\n",
    "\n",
    "bad_match = {(661, 801), (228, 388), (304, 735), (101, 102)}\n",
    "print(\"Precision: \", precision(bad_match))\n",
    "print(\"Recall: \", recall(bad_match))\n",
    "print(\"F1 score: \", f1_score(bad_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem E\n",
    "As you can see, the match above is pretty poor; it covers less than 5% of the pairs that should really be merged. Your task is to compute a better match. For full credit (100%), your match should achieve an F1 score of at least 0.93. Your code below should compute your match, and then print the F1 score achieved (you do not need to print the entire match itself). After that, you should describe in a few sentences what your record linkage code is doing. Note that there is no one \"correct\" way to compute a good match, you should feel free to use any of the information contained in `df_a` and `df_b`, along with the tools above. \n",
    "\n",
    "If you need help getting started, here are a few hints:\n",
    "- You can always start with the records that have exact matches on names.\n",
    "- You can also use the similarity methods from above with different parameters to get mispelled names.\n",
    "- Apart from the names and ids, you can look at the city, address, and type of restaurant for every record, and try to match on those as well.\n",
    "- You can always compute multiple matches and then combine them.\n",
    "- Check out the precision and recall of your results. If precision is low but recall is high, you are trying to link too many pairs. If recall is low, but precision is high, you are not trying to link enough paris.\n",
    "- If you are completely stuck, the \"answers\" (that is, the ground truth matches) are included in `hw3_utils.py`, so you can try checking out the ones you are missing to find out why your code isn't capturing them. Note that you will not receive credit for hard coding these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9655172413793104\n",
      "Recall:  0.9180327868852459\n",
      "F1 score:  0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "mySet = set()\n",
    "\n",
    "for a in range(0, len(df_a)):\n",
    "    for b in range(0, len(df_b)):\n",
    "        nameA = df_a[\"name\"][a]\n",
    "        nameB = df_b[\"name\"][b]\n",
    "        cityA = df_a[\"city\"][a]\n",
    "        cityB = df_b[\"city\"][b]\n",
    "        typeA = df_a[\"type\"][a]\n",
    "        typeB = df_b[\"type\"][b]\n",
    "        addA = df_a[\"address\"][a]\n",
    "        addB = df_b[\"address\"][b]\n",
    "        idA = df_a[\"id\"][a]\n",
    "        idB = df_b[\"id\"][b]\n",
    "        d = jac(nameA, nameB)\n",
    "        e = jac(addA, addB)\n",
    "        f = jac(cityA, cityB)\n",
    "        c = jac(typeA, typeB)\n",
    "        if d <= 1 and d >= 0.4:\n",
    "            if f <= 1 and f >= 0.4:\n",
    "                mySet.add((idA, idB))\n",
    "        di = edit_dist(nameA, nameB)\n",
    "        if e <= 1 and e >= 0.5:\n",
    "            if di >= 1 and di <= 2:\n",
    "                mySet.add((idA, idB))\n",
    "        if f <= 1 and f >= 0.5:\n",
    "            if di >= 0 and di <= 2:\n",
    "                mySet.add((idA, idB))\n",
    "        if b <= 1 and b >= 0.5:\n",
    "            if di >= 0 and di <= 2:\n",
    "                mySet.add((idA, idB))\n",
    "        if e == 1:\n",
    "            if f == 1:\n",
    "                if c == 1:\n",
    "                    mySet.add((idA, idB))\n",
    "        \n",
    "print(\"Precision: \", precision(mySet))\n",
    "print(\"Recall: \", recall(mySet))\n",
    "print(\"F1 score: \", f1_score(mySet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we added every pair whose names had a jacard similarity >= 0.4 and <= 1 and whose city names had a jacard similarity >= 0.4 and <= 1.  Then we added all pairs whos names had an edit distance of >= 1 and <= 2 and whose city names, addresses, and types had a jacard similarity >= 0.5.  Lastly, we added every pair that had the exact same address, type, and city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting HW 3 \n",
    "1. Double check that you have written all of your answers along with your supporting work in this notebook. Make sure you save the complete notebook.\n",
    "1. Double check that your entire notebook runs correctly and generates the expected output. To do so, you can simply select Kernel -> Restart and Run All. \n",
    "2. You will download two versions of your notebook to submit, a .pdf and a .py. To create a PDF, we reccomend that you select File --> Download as --> HTML (.html). Open the downloaded .html file; it should open in your web broser. Double check that it looks like your notebook, then print a .pdf using your web browser (you should be able to select to print to a pdf on most major web browsers and operating systems). Check your .pdf for readability: If some long cells are being cut off, go back to your notebook and split them into multiple smaller cells. To get the .py file from your notebook, simply select File -> Download as -> Python (.py) (note, we recognize that you may not have written any Python code for this assignment, but will continue the usual workflow for consistency). \n",
    "3. Upload the .pdf to gradescope under hw3 report and the .py to gradescope under hw3 code. If you work with a partner, only submit one document for both of you, but be sure to add your partner using the [group feature on gradescope](https://www.gradescope.com/help#help-center-item-student-group-members)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
