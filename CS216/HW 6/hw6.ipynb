{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6\n",
    "__CS 216, Everything Data, Spring 2020__\n",
    "\n",
    "__DUE: Monday Mar. 2 by 4:40 pm (class time)__\n",
    "\n",
    "__Joe Cusano (jgc28) and Pierce Forte (ph7)__\n",
    "\n",
    "In this assignment, you will explore more machine learning tools for prediction using the scikit-learn library. You will include all of your answers for this assignment within this notebook. You will then convert your notebook to a .pdf and a .py file to submit to gradescope (submission instructions are included at the bottom).\n",
    "\n",
    "Please take note of the [course collaboration policy](https://sites.duke.edu/compsci216s2020/policies/). You may work alone or with a single partner. If you work with a partner, you may not split up the assignment; you should work together in-person or complete parts independently and come together to discuss your solutions. In either case, you are individually responsible for your work, and should understand everything in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Getting Started with Scikit-Learn\n",
    "Scikit-learn is the standard open source library for prediction in Python. To get started, visit https://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting and read through the learning and prediction section. Once you have read the text, go through the following simple example to train and test a predictive model on the iris dataset from HW 5. Note that this example will essentially do *exactly* what your implementation from HW 5 does (up to a different random partition of the data), but does it using scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the iris dataset from HW 5 and convert to Numpy arrays\n",
    "df_iris = pd.read_csv('iris.csv')\n",
    "X_iris = df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y_iris = df_iris['flower_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use scikit-learn to split data into train and test subsets\n",
    "# First param is the X values we want to use for prediction\n",
    "# Second param is the y values we want to predict\n",
    "# test_size is the fraction of the data reserved for the test set\n",
    "# shuffle = True if you want to randomly split the data into test/train\n",
    "# random_state can be set if you want to get the same split on multiple runs\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, \n",
    "                                                                        test_size=0.33, shuffle=True, \n",
    "                                                                        random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize our classifier and set any parameters (k, for the knn algorithm)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# We call the fit method on a classifier and pass it training data in order \n",
    "# to train the model\n",
    "knn.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# Finally, the predict method on a classifier, given test data, will give us\n",
    "# predicted y values.\n",
    "knn_prediction = knn.predict(X_iris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "# The metrics module in scikit-learn contains a variety of \n",
    "# ways to evaluate our predictions. The simplest is to just \n",
    "# get the accuracy (overall fraction of predictions that are correct)\n",
    "print(\"Classification accuracy: \", metrics.accuracy_score(y_iris_test, knn_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read more about scikit-learn before continuing, visit the Getting Started page: https://scikit-learn.org/stable/getting_started. You can find the full documentation in the user guide: https://scikit-learn.org/stable/user_guide.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem A\n",
    "Recall that we discussed the support vector machine as a powerful classifier during lecture. Use the scikit-learn implementation of support vector machines to make class predictions for the iris dataset as above, and measure the accuracy of your predictions. You can find the documentation of support vector machines here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Write your code for Problem A here\n",
    "clf = SVC(gamma = \"auto\")\n",
    "clf.fit(X_iris_train, y_iris_train)\n",
    "clf_prediction = clf.predict(X_iris_test)\n",
    "\n",
    "print(\"Classification accuracy: \", metrics.accuracy_score(y_iris_test, clf_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regression\n",
    "So far, we have built classifiers that predict a categorical variable. Regression is the branch of prediction that attempts to predict numerical variables. A common example is predicting prices. In this part, we examine building a regression model to predict house prices. Specifically, the following dataset contains information about neighborhoods in the city of Boston in the year 1978. Each row corresponds to a given neighborhood. The final column contains the median house price in thousands of dollars (homes used to be a lot cheaper), and the other 12 columns contain information about the neighborhood (you can read the interpretation of these columns in the text file `housing.names` if interested). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "df_boston = pd.read_table('housing.data', delim_whitespace=True, header = None)\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we split our data into training and test sets\n",
    "X_boston = df_boston.iloc[:,0:13].values\n",
    "y_boston = df_boston.iloc[:,13].values\n",
    "X_boston_train, X_boston_test, y_boston_train, y_boston_test = train_test_split(X_boston, y_boston, \n",
    "                                                                                test_size=0.5, shuffle=True, \n",
    "                                                                                random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem B\n",
    "Train an ordinary least squares regression model on the boston housing data that predicts median house price `y` based on numerical attributes of the nieghborhoods `X`. You can find the relevant scikit-learn documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.  Note that there are no parameters to tune. After you have trained your model using `X_boston_train` and `y_boston_train`, make predictions based on `X_boston_test` and compare them against `y_boston_test`. To evaluate your predictions, compute the mean squared error of your predictions (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.839287038640286"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Write your code for problem B here\n",
    "reg = LinearRegression().fit(X_boston_train,y_boston_train)\n",
    "predictions = reg.predict(X_boston_test)\n",
    "mean_squared_error(y_boston_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem C\n",
    "In this problem, we consider a different dataset, `mystery_X` and `mystery_y`. Try repeating what you did in Problem B for this dataset, learning an ordinary least squares model with `y_myst_train` and `X_myst_train`predicting `y_myst_test` based on `X_myst_test`. Report the mean squared error that you get. Explain why the error is so high, and then learn a different regression model that achieves mean squared error less than 100. You can use whatever regression tools you like, but we recommend trying different kernels using kernel ridge regression: https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_myst = np.loadtxt('mystery_X.csv', delimiter=',')\n",
    "y_myst = np.loadtxt('mystery_y.csv', delimiter=',')\n",
    "\n",
    "X_myst_train, X_myst_test, y_myst_train, y_myst_test = train_test_split(X_myst, y_myst, \n",
    "                                                                        test_size=0.5, shuffle=True, \n",
    "                                                                        random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033.40593033121\n",
      "36.40433183105104\n"
     ]
    }
   ],
   "source": [
    "# Write your code for problem C here\n",
    "reg = LinearRegression().fit(X_myst_train, y_myst_train)\n",
    "predictions = reg.predict(X_myst_test)\n",
    "print(mean_squared_error(y_myst_test, predictions))\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "clf = KernelRidge(kernel=\"polynomial\")\n",
    "clf.fit(X_myst_train, y_myst_train)\n",
    "predictions = clf.predict(X_myst_test)\n",
    "print(mean_squared_error(y_myst_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inital error is so high because a linear relationship cannot be discovered in the original dataset without transforming the variables.  We used the kernel trick in order to implicitly transform the data and reduce the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 3: Cross Validation and Model Evaluation\n",
    "So far, we have not worried too much about how to set model hyperparameters or how to evaluate models. For example, in the Part 1 we somewhat arbitrarily set the value of k for the k-nn algorithm to 5. How would you find a good value in general? Also in Part 1, we just evaluated our model by computing the overall accuracy on the test data, but that doesn't tell us much about *what kind of errors* our model is making. We will work with the movielens review dataset from HW 4, but we have processed the data into the file `movie_ratings.csv` so that it includes, for every user (that is, every individual who reviewed movies), their sex (1 for female, 0 for male) and a column for every genre of movie containing the number of movies of that genre they rated. Our goal will be to predict sex based on the number of reviews of different genres, which is a substantially harder prediction task than the flower types above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>unknown</th>\n",
       "      <th>action</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>children</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>documentary</th>\n",
       "      <th>drama</th>\n",
       "      <th>fantasy</th>\n",
       "      <th>film_noir</th>\n",
       "      <th>horror</th>\n",
       "      <th>musical</th>\n",
       "      <th>mystery</th>\n",
       "      <th>romance</th>\n",
       "      <th>sci_fi</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  unknown  action  adventure  animation  children  comedy  crime  \\\n",
       "0    0        0       0          0          0         0       0      0   \n",
       "1    1        0       0          0        272       272     272      0   \n",
       "2    0        0      62         62          0         0       0      0   \n",
       "3    0        0       0          0          0         0       0      0   \n",
       "4    1        0      24          0          0         0      24      0   \n",
       "\n",
       "   documentary  drama  fantasy  film_noir  horror  musical  mystery  romance  \\\n",
       "0            0      0        0          0       0        0        0        0   \n",
       "1            0      0        0          0       0        0        0        0   \n",
       "2            0      0        0          0       0        0        0        0   \n",
       "3            0      0        0          0       0        0        0        0   \n",
       "4            0     24        0          0       0        0        0        0   \n",
       "\n",
       "   sci_fi  thriller  war  western  \n",
       "0       0         0    0        0  \n",
       "1       0         0    0        0  \n",
       "2       0        62    0        0  \n",
       "3       0        54    0        0  \n",
       "4       0         0    0        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts = pd.read_csv('movie_ratings.csv')\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we split our data into training and test sets\n",
    "X_movie = df_counts.iloc[:,1:].values\n",
    "y_movie = df_counts['sex'].values\n",
    "X_movie_train, X_movie_test, y_movie_train, y_movie_test = train_test_split(X_movie, y_movie, \n",
    "                                                                            test_size=0.33, shuffle=True, \n",
    "                                                                            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try using a decision tree for this prediction task (recall the idea from HW 5). We need to set a hyperparameter, namely, the `max_depth` parameter which determines how many layers we will let our decision tree have. A naive way to tune a parameter like `max_depth` is to try different settings, each time training on the train data and then testing the accuracy on the train data (note that we never want to tune parameters based on the test data, that is considered *data leakage*). Below, we try a few different values for `max_depth` using this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy with max_depth= 1  is:  0.716323296354992\n",
      "Training accuracy with max_depth= 2  is:  0.7258320126782885\n",
      "Training accuracy with max_depth= 4  is:  0.7353407290015848\n",
      "Training accuracy with max_depth= 8  is:  0.7812995245641838\n",
      "Training accuracy with max_depth= 16  is:  0.8573692551505546\n",
      "Training accuracy with max_depth= 32  is:  0.96513470681458\n",
      "Training accuracy with max_depth= 64  is:  0.9698890649762282\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "param_search_space = [1, 2, 4, 8, 16, 32, 64]\n",
    "for param in param_search_space:\n",
    "    dt = tree.DecisionTreeClassifier(max_depth=param)\n",
    "    dt.fit(X_movie_train, y_movie_train)\n",
    "    pred_movie_train = dt.predict(X_movie_train)\n",
    "    print('Training accuracy with max_depth=', param, \" is: \", metrics.accuracy_score(y_movie_train, pred_movie_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this testing, we conclude that clearly bigger is better; we get almost perfect accuracy on the training set by using a very large decision tree. What happens when we then make predictions and measure our accuracy on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with max_depth= 64  is:  0.6121794871794872\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=64)\n",
    "dt.fit(X_movie_train, y_movie_train)\n",
    "pred_movie_test = dt.predict(X_movie_test)\n",
    "print('Test accuracy with max_depth=', 64, \" is: \", metrics.accuracy_score(y_movie_test, pred_movie_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classic case of overfitting: we built a model that was too complex, enough so that it was able to more or less \"memorize\" the training data but doesn't generalize to examples it wasn't trained on. Maybe we could change `max_depth` to avoid this problem. \n",
    "\n",
    "A standard approach to finding a good setting for a hyperparameter is cross-validation. We take our training data (*only* the training data; tuning model parameters using test data is a prime example of data leakage in modeling), and create many random partitions of the data into train/validate sets. For each partition, we train the model (with a given hyperparameter setting) on the train set, and then test on the validate set. We can then average (or otherwise compare) the results on all of the random partitions, to ensure that the parameter setting wasn't just good for a particular lucky split of the data. We do this for many parameter settings, and take the best as our setting to use for training our model. You can find the documentation for the scikit-learn implementation of cross validation here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation avg accuracy with max_depth= 1  is:  0.7067968503937007\n",
      "Cross validation avg accuracy with max_depth= 2  is:  0.7035968503937007\n",
      "Cross validation avg accuracy with max_depth= 4  is:  0.6987968503937008\n",
      "Cross validation avg accuracy with max_depth= 8  is:  0.6908472440944882\n",
      "Cross validation avg accuracy with max_depth= 16  is:  0.6797228346456694\n",
      "Cross validation avg accuracy with max_depth= 32  is:  0.6133291338582677\n",
      "Cross validation avg accuracy with max_depth= 64  is:  0.6132535433070866\n"
     ]
    }
   ],
   "source": [
    "for param in param_search_space: \n",
    "    dt = tree.DecisionTreeClassifier(max_depth=param)\n",
    "    dt_cv = cross_validate(dt, X_movie_train, y_movie_train, cv=5)\n",
    "    print('Cross validation avg accuracy with max_depth=', param, \" is: \", dt_cv['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this testing, we conclude that clearly smaller is better; let's try `max_depth` of 2, which seems to do better in cross validation, which suggests that it's test performance will be better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with max_depth=2 is:  0.7115384615384616\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
    "dt.fit(X_movie_train, y_movie_train)\n",
    "pred_movie_test = dt.predict(X_movie_test)\n",
    "print('Test accuracy with max_depth=2 is: ', metrics.accuracy_score(y_movie_test, pred_movie_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a standard technique that allows us to find a better setting of hyperparameters without risking data leakage by tuning the parameters to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem D\n",
    "So far, we have only evaluated the overall classification accuracy (that is, the fraction of classifications we got correct) to measure the quality of our model. But something strange is going on here. Suppose we want to check separately how good our predictions are for class 0 and for class 1 (that is, for those with `sex=0` and `sex=1` separately). Recall from our discussion of property testing in record linkage that we can compute the *f1 score* for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score for class 1 with max_depth=2 is:  0.0425531914893617\n",
      "Test f1 score for class 0 with max_depth=2 is:  0.830188679245283\n"
     ]
    }
   ],
   "source": [
    "print('Test f1 score for class 1 with max_depth=2 is: ', metrics.f1_score(y_movie_test, pred_movie_test))\n",
    "print('Test f1 score for class 0 with max_depth=2 is: ', metrics.f1_score(1-y_movie_test, 1-pred_movie_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on here? Why are our predictions so much worse for class 1 than class 0, and why did we learn a model that performs so differently for the two classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score for class 1 with max_depth=16 is:  0.2074074074074074\n",
      "Test f1 score for class 0 with max_depth=16 is:  0.7811860940695295\n",
      "Test f1 score for class 1 with max_depth=32 is:  0.26506024096385544\n",
      "Test f1 score for class 0 with max_depth=32 is:  0.7336244541484717\n",
      "Test f1 score for class 1 with max_depth=32 is:  0.2738095238095238\n",
      "Test f1 score for class 0 with max_depth=32 is:  0.7324561403508772\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=16)\n",
    "dt.fit(X_movie_train, y_movie_train)\n",
    "pred_movie_test = dt.predict(X_movie_test)\n",
    "print('Test f1 score for class 1 with max_depth=16 is: ', metrics.f1_score(y_movie_test, pred_movie_test))\n",
    "print('Test f1 score for class 0 with max_depth=16 is: ', metrics.f1_score(1-y_movie_test, 1-pred_movie_test))\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=32)\n",
    "dt.fit(X_movie_train, y_movie_train)\n",
    "pred_movie_test = dt.predict(X_movie_test)\n",
    "print('Test f1 score for class 1 with max_depth=32 is: ', metrics.f1_score(y_movie_test, pred_movie_test))\n",
    "print('Test f1 score for class 0 with max_depth=32 is: ', metrics.f1_score(1-y_movie_test, 1-pred_movie_test))\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(max_depth=128)\n",
    "dt.fit(X_movie_train, y_movie_train)\n",
    "pred_movie_test = dt.predict(X_movie_test)\n",
    "print('Test f1 score for class 1 with max_depth=128 is: ', metrics.f1_score(y_movie_test, pred_movie_test))\n",
    "print('Test f1 score for class 0 with max_depth=128 is: ', metrics.f1_score(1-y_movie_test, 1-pred_movie_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that when the model is only given a max_depth of 2, the model is underfitted and selects a few features that work very well for class 1 while not working as well for class 0.  These features cause the model to perform well overall, but not to perform well for class 0.  As the depth is increased and more features are selected into the model, the f1 score for class 0 increases as more features that work well for class 0 are selected.  It's also possible that there is a greater proportion of class 0 people in the training set, so that when the model performs better overall when it selects for class 0 more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Gentle Introduction to Deep Learning\n",
    "Recall that in lecture we discussed the multilayer perceptron as the conceptually simplest neural network architecture for deep learning. In this part, you will train a multilayer perceptron for image recognition. Below, we load a small subset of the MNIST dataset containing hundreds of images of handwritten digits 0 through 9. Each image is stored as a low-resolution 28 by 28 grayscale image (we plot an example below). We flatten these into a single vector of length 784. Our goal is to learn a model that predicts the digit given the pixel values for the handwritten image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_digits_train, X_digits_test, y_digits_train, y_digits_test = train_test_split(data, digits.target, \n",
    "                                                                                test_size=0.33, shuffle=True, \n",
    "                                                                                random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem E\n",
    "Below we have initialized a multilayer perceptron using the scikit-learn implementation. We set the random state parameter so that we get consistent results every time we train, and then we specified an exceedingly arbitrary network configuration. Your first task is to tune the parameters of the classifier so that it achieves an average classification accuracy of at least 0.97 in 5-fold cross validation on the training data (which is computed for you below). You can find the documentation for the scikit-learn implementation here: https://scikit-learn.org/stable/modules/neural_networks_supervised.html. Some tips:\n",
    "- The parameter `hidden_layer_sizes` is a tuple where the i'th value gives the number of neurons in the i'th layer of the network. So, for example, `hidden_layer_sizes = (40, 20)` would correspond to a network with two hidden layers, the first having 40 neurons and the second having 20. The default is a single hidden layer with 100 neurons.\n",
    "- If you get a warning that the solver did not converge, you can either change some of the parameter settings, or increase the maximum number of iterations of optimization by changing the parameter `max_iter` (default is 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.9725834935996857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# TO DO - Fix our neural network so that it achieves at least 0.97 accuracy \n",
    "# on average in 5-fold cross validation. You can change any parameters \n",
    "# except the random_state \n",
    "net = MLPClassifier(random_state=1, hidden_layer_sizes=(220, 250), max_iter = 200)\n",
    "net_cv = cross_validate(net, X_digits_train, y_digits_train)\n",
    "print(\"Average accuracy: \", net_cv['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have tuned your model using cross validation on the training data, make predictions for the test data. Compute and report the accuracy of your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is:  0.9848484848484849\n"
     ]
    }
   ],
   "source": [
    "net.fit(X_digits_train, y_digits_train)\n",
    "predictions = net.predict(X_digits_test)\n",
    "print('Prediction accuracy is: ', metrics.accuracy_score(y_digits_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that multilayer perceptrons are *not* the state of the art solution for image classification tasks like this. Instead, the standard neural network architecture for these sorts of task is the *convolutional neural network*. At this point, we reach the limit of what is implemented for you in scikit-learn. If you want to go further with deep learning, you will need to look beyond scikit-learn. Fortunately, there are multiple professional-quality open-source libraries for deep learning that work well with Python. Two of the most popular and powerful currently are:\n",
    "- PyTorch: https://pytorch.org \n",
    "- and TensorFlow https://www.tensorflow.org\n",
    "\n",
    "While CS 216 is not a course on deep learning specifically, you should feel free (encouraged, in fact) to explore these libraries further if you would like to incorporate deep learning into your course project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting HW 6\n",
    "1. Double check that you have written all of your answers along with your supporting work in this notebook. Make sure you save the complete notebook.\n",
    "1. Double check that your entire notebook runs correctly and generates the expected output. To do so, you can simply select Kernel -> Restart and Run All. \n",
    "2. You will download two versions of your notebook to submit, a .pdf and a .py. To create a PDF, we reccomend that you select File --> Download as --> HTML (.html). Open the downloaded .html file; it should open in your web broser. Double check that it looks like your notebook, then print a .pdf using your web browser (you should be able to select to print to a pdf on most major web browsers and operating systems). Check your .pdf for readability: If some long cells are being cut off, go back to your notebook and split them into multiple smaller cells. To get the .py file from your notebook, simply select File -> Download as -> Python (.py) (note, we recognize that you may not have written any Python code for this assignment, but will continue the usual workflow for consistency). \n",
    "3. Upload the .pdf to gradescope under hw 6 report and the .py to gradescope under hw 6 code. If you work with a partner, only submit one document for both of you, but be sure to add your partner using the [group feature on gradescope](https://www.gradescope.com/help#help-center-item-student-group-members)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
